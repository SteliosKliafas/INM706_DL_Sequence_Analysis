{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INM706 Deep Learning for Sequence Analysis\n",
    "### Sarah Rhalem (190051884) & Stelios Kliafas (########)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draft Notes/ Working comments:\n",
    "Dataset >> Problem +Evaluation Metric >> Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, dataloader\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Set to use GPU on device if available:\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\sarah\\\\Documents\\\\MSc AI 2020_2021\\\\INM706\\\\INM706_DL_Sequence_Analysis'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dramas                          1384\n",
       "Comedies                        1074\n",
       "Documentaries                    751\n",
       "Action & Adventure               721\n",
       "International TV Shows           690\n",
       "Children & Family Movies         502\n",
       "Crime TV Shows                   369\n",
       "Kids' TV                         359\n",
       "Stand-Up Comedy                  321\n",
       "Horror Movies                    244\n",
       "British TV Shows                 232\n",
       "Docuseries                       194\n",
       "Anime Series                     148\n",
       "International Movies             114\n",
       "TV Comedies                      110\n",
       "Reality TV                       102\n",
       "Classic Movies                    77\n",
       "TV Dramas                         62\n",
       "Movies                            56\n",
       "Thrillers                         49\n",
       "TV Action & Adventure             37\n",
       "Stand-Up Comedy & Talk Shows      33\n",
       "Romantic TV Shows                 28\n",
       "Classic & Cult TV                 21\n",
       "Independent Movies                20\n",
       "Anime Features                    19\n",
       "Music & Musicals                  17\n",
       "Cult Movies                       12\n",
       "TV Shows                          12\n",
       "Sci-Fi & Fantasy                  11\n",
       "TV Horror                         10\n",
       "Romantic Movies                    3\n",
       "Spanish-Language TV Shows          2\n",
       "LGBTQ Movies                       1\n",
       "Sports Movies                      1\n",
       "TV Sci-Fi & Fantasy                1\n",
       "Name: listing, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load csv dataset, create listing column\n",
    "raw_dataset_df= pd.read_csv(os.path.join(\"Data\\\\netflix_titles.csv\") , encoding=\"utf8\")\n",
    "raw_dataset_df[\"listing\"]= raw_dataset_df[\"listed_in\"].str.split(pat=\",\", n=1).str.get(0)\n",
    "\n",
    "# Cleanse Data\n",
    "raw_dataset_df[\"description\"].isna().sum() # Check null entries for description - None\n",
    "raw_dataset_df[\"plot_description\"]=raw_dataset_df[\"description\"].map(lambda x: re.sub( r'\"', '', x)) # (TO BE UPDATED REMOVES QUOTATION MARKS)\n",
    "\n",
    "raw_dataset_df.listing.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           genre                                   plot_description\n",
      "0  international  In a future where the elite inhabit an island ...\n",
      "1          drama  After a devastating earthquake hits Mexico Cit...\n",
      "2         horror  When an army recruit is found dead, his fellow...\n",
      "3         action  In a postapocalyptic world, rag-doll robots hi...\n",
      "4          drama  A brilliant group of students become card-coun...\n",
      "['international' 'drama' 'horror' 'action' 'crime' 'documentary' 'other'\n",
      " 'comedy' 'anime' 'children' 'romance']\n"
     ]
    }
   ],
   "source": [
    "# Map each data sample listing to a generic genre\n",
    " \n",
    " # Identify the show listings for mapping to summarised genres\n",
    "raw_dataset_df.listing.value_counts()\n",
    "\n",
    "# map show listing to a specific genre. Note: Listing types with under ~100 data samples are classified under the genre \"Other\"\n",
    "genre_mapping= { \"romance\": {\"Romantic TV Shows\", \"Romantic Movies\"} ,\n",
    "                \"drama\": {\"Dramas\", \"TV Dramas\"}  ,\n",
    "                 \"comedy\": {\"Comedies\", \"Stand-Up Comedy\", \"TV Comedies\", \"Stand-Up Comedy & Talk Shows\"},\n",
    "                 \"documentary\": {\"Documentaries\", \"Docuseries\"},\n",
    "                 \"action\": {\"Action & Adventure\", \"TV Action & Adventure\"} ,\n",
    "                 \"international\": {\"International TV Shows\", \"International Movies\", \"Spanish-Language TV Shows\"},\n",
    "                 \"children\": {\"Children & Family Movies\", \"Kids' TV\"},\n",
    "                 \"crime\": {\"Crime TV Shows\"},\n",
    "                 \"horror\": {\"Horror Movies\", \"TV Horror\"} ,\n",
    "                 \"anime\" : {\"Anime Series\", \"Anime Features\"},\n",
    "                 \"other\" : {\"Thrillers\", \"British TV Shows\", \"Reality TV\", \"Classic & Cult TV\", \"TV Shows\", \"TV Sci-Fi & Fantasy\",\n",
    "                         \"Classic Movies\", \"Movies\", \"Independent Movies\", \"Cult Movies\", \"Sports Movies\", \"LGBTQ Movies\", \"Music & Musicals\",\n",
    "                         \"Sci-Fi & Fantasy\"} }\n",
    "\n",
    "# function to map listings to genres by dictionary key\n",
    "def map_function(dictionary):\n",
    "    def my_map(x):\n",
    "        res = \"\"\n",
    "        for key in dictionary.keys():\n",
    "            if (x in dictionary[key]):\n",
    "                res = key\n",
    "                break\n",
    "        return res\n",
    "    return my_map\n",
    "\n",
    "# Add genre column based on listing mapping\n",
    "raw_dataset_df[\"genre\"] = raw_dataset_df[\"listing\"].map(map_function(genre_mapping))\n",
    "\n",
    "# Write to txt file\n",
    "plot_dataset_df= raw_dataset_df[[\"genre\",\"plot_description\"]].copy()\n",
    "plot_dataset= plot_dataset_df.to_csv('Data\\\\netflix_plot_dataset.txt', index=False, header=None, sep=';' )\n",
    "\n",
    "# Sense check - view data header and check all descriptions were mapped\n",
    "print(plot_dataset_df.head())\n",
    "print(plot_dataset_df.genre.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 14 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50271, 768)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Load model and Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model= GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model= model.to(device)   \n",
    "    \n",
    "special_tokens_dict = {\n",
    "                \"bos_token\": \"<BOS>\",\n",
    "                \"eos_token\": \"<EOS>\",\n",
    "                \"pad_token\": \"<PAD>\",\n",
    "                \"additional_special_tokens\": [\n",
    "                   \"<romance>\",\n",
    "                    \"<drama>\",\n",
    "                    \"<comedy>\",\n",
    "                    \"<documentary>\",\n",
    "                    \"<action>\",\n",
    "                    \"<international>\",\n",
    "                    \"<children>\",\n",
    "                    \"<crime>\",\n",
    "                    \"<horror>\",\n",
    "                    \"<anime>\",\n",
    "                    \"<other>\",\n",
    "                ],\n",
    "            }\n",
    "\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print('We have added', num_added_toks, 'tokens')\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class NetflixPlotDataset(Dataset):\n",
    "          def __init__(self, tokenizer=tokenizer, dataset_path=os.path.join(\"Data\\\\netflix_plot_dataset.txt\"), block_size=100): # block_size missing\n",
    "   \n",
    "              with open(dataset_path, encoding=\"utf-8\") as f:\n",
    "                      lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
    "   \n",
    "              self.examples = tokenizer.batch_encode_plus(lines, add_special_tokens=True, max_length=block_size)[\"input_ids\"]\n",
    "   \n",
    "          def __len__(self):\n",
    "            return len(self.examples)\n",
    "\n",
    "          def __getitem__(self, i):\n",
    "             return torch.tensor(self.examples[i], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=NetflixPlotDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[45609,    26,   818,   257,  2003,   810,   262,  9085, 14527,   281,\n",
       "          7022, 31354,  1290,   422,   262, 18012,  1017,  5700,    11,   345,\n",
       "           651,   530,  2863,   284,  4654,   262,   513,     4,  7448,   422,\n",
       "          2809,   282,   273,    13]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IGNORE BELOW (WORKING NOTES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "sentence = \"I am french\"\n",
    "input_ids= tokenizer.encode(sentence, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   40,   716, 48718])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(input_ids, max_length=100, num_beans=5, no_repeat_ngram_size=2, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am french, I am a French person. I have a lot of friends in France, and I'm very proud of them.\\n\\nI'm a very good person, but I don't know how to express myself. It's not easy to do. But I know that I can do it. And I want to be able to. So I've been doing this for a long time.\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
