{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjzZrB9zd_F1"
   },
   "source": [
    "# INM706 Deep Learning for Sequence Analysis\n",
    "### Sarah Rhalem (190051884) & Stelios Kliafas (########)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpkCS212d_GQ"
   },
   "source": [
    "Draft Notes/ Working comments:\n",
    "Dataset >> Problem +Evaluation Metric >> Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cctN2saeTaC",
    "outputId": "0941f7a7-9b78-4709-fe72-1a3c379fb32f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\python\\python38\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: filelock in c:\\python\\python38\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python\\python38\\lib\\site-packages (from transformers) (1.18.5)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: packaging in c:\\python\\python38\\lib\\site-packages (from transformers) (20.8)\n",
      "Requirement already satisfied: sacremoses in c:\\python\\python38\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\python\\python38\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\python\\python38\\lib\\site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: requests in c:\\python\\python38\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python\\python38\\lib\\site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\python\\python38\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: click in c:\\python\\python38\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in c:\\python\\python38\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in c:\\python\\python38\\lib\\site-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\python\\python38\\lib\\site-packages (from requests->transformers) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yV4GMpe1_qrC",
    "outputId": "c045ea1d-ffaf-4b41-d7f1-e25f033d3f5d"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6RAnpjawd_GT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SequentialSampler, RandomSampler\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajWl3nZieSOe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G8zqkulrd_GV",
    "outputId": "29a6af4a-d753-4f68-819b-09f31cc9fe00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Set to use GPU on device if available:\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "nQseXoIzd_GZ",
    "outputId": "2a867c3c-1d56-4775-ff0b-861b54f6987f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\public2\\\\Desktop\\\\INM706_DL_Sequence_Analysis'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-L_kdVVTd_Gc",
    "outputId": "de53a325-5980-4df6-90ae-e5781280c466"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dramas                    24\n",
       "Comedies                  15\n",
       "Action & Adventure        11\n",
       "Documentaries             11\n",
       "Horror Movies             11\n",
       "International TV Shows     8\n",
       "Crime TV Shows             7\n",
       "British TV Shows           2\n",
       "Movies                     2\n",
       "Sports Movies              1\n",
       "Reality TV                 1\n",
       "TV Comedies                1\n",
       "Anime Series               1\n",
       "International Movies       1\n",
       "Sci-Fi & Fantasy           1\n",
       "Docuseries                 1\n",
       "Independent Movies         1\n",
       "Name: listing, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load csv dataset, create listing column\n",
    "raw_dataset_df= pd.read_csv(os.path.join(\"Data\\\\testing_dataset.csv\") , encoding=\"utf8\")\n",
    "raw_dataset_df[\"listing\"]= raw_dataset_df[\"listed_in\"].str.split(pat=\",\", n=1).str.get(0)\n",
    "\n",
    "# Cleanse Data\n",
    "raw_dataset_df[\"description\"].isna().sum() # Check null entries for description - None\n",
    "raw_dataset_df[\"plot_description\"]=raw_dataset_df[\"description\"].map(lambda x: re.sub( r'\"', '', x)) # (TO BE UPDATED REMOVES QUOTATION MARKS)\n",
    "\n",
    "raw_dataset_df.listing.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Ur4lFS4id_Ge",
    "outputId": "998313be-736e-42ac-91a3-2be5dc4d7d24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In a future where the elite inhabit an island paradise far from the crowded slums, you get one chance to join the 3% saved from squalor.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset_df[\"description\"].to_list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zjDc-SrGd_Gf",
    "outputId": "29af6a57-136d-4752-b63d-0d0e2c9f90fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             genre                                   plot_description\n",
      "0  <international>  In a future where the elite inhabit an island ...\n",
      "1          <drama>  After a devastating earthquake hits Mexico Cit...\n",
      "2         <horror>  When an army recruit is found dead, his fellow...\n",
      "3         <action>  In a postapocalyptic world, rag-doll robots hi...\n",
      "4          <drama>  A brilliant group of students become card-coun...\n",
      "['<international>' '<drama>' '<horror>' '<action>' '<crime>'\n",
      " '<documentary>' '<other>' '<comedy>' '<anime>']\n"
     ]
    }
   ],
   "source": [
    "# Map each data sample listing to a generic genre\n",
    " \n",
    " # Identify the show listings for mapping to summarised genres\n",
    "raw_dataset_df.listing.value_counts()\n",
    "\n",
    "# map show listing to a specific genre. Note: Listing types with under ~100 data samples are classified under the genre \"Other\"\n",
    "genre_mapping= { \"<romance>\": {\"Romantic TV Shows\", \"Romantic Movies\"} ,\n",
    "                \"<drama>\": {\"Dramas\", \"TV Dramas\"}  ,\n",
    "                 \"<comedy>\": {\"Comedies\", \"Stand-Up Comedy\", \"TV Comedies\", \"Stand-Up Comedy & Talk Shows\"},\n",
    "                 \"<documentary>\": {\"Documentaries\", \"Docuseries\"},\n",
    "                 \"<action>\": {\"Action & Adventure\", \"TV Action & Adventure\"} ,\n",
    "                 \"<international>\": {\"International TV Shows\", \"International Movies\", \"Spanish-Language TV Shows\"},\n",
    "                 \"<children>\": {\"Children & Family Movies\", \"Kids' TV\"},\n",
    "                 \"<crime>\": {\"Crime TV Shows\"},\n",
    "                 \"<horror>\": {\"Horror Movies\", \"TV Horror\"} ,\n",
    "                 \"<anime>\" : {\"Anime Series\", \"Anime Features\"},\n",
    "                 \"<other>\" : {\"Thrillers\", \"British TV Shows\", \"Reality TV\", \"Classic & Cult TV\", \"TV Shows\", \"TV Sci-Fi & Fantasy\",\n",
    "                         \"Classic Movies\", \"Movies\", \"Independent Movies\", \"Cult Movies\", \"Sports Movies\", \"LGBTQ Movies\", \"Music & Musicals\",\n",
    "                         \"Sci-Fi & Fantasy\"} }\n",
    "\n",
    "# function to map listings to genres by dictionary key\n",
    "def map_function(dictionary):\n",
    "    def my_map(x):\n",
    "        res = \"\"\n",
    "        for key in dictionary.keys():\n",
    "            if (x in dictionary[key]):\n",
    "                res = key\n",
    "                break\n",
    "        return res\n",
    "    return my_map\n",
    "\n",
    "# Add genre column based on listing mapping\n",
    "raw_dataset_df[\"genre\"] = raw_dataset_df[\"listing\"].map(map_function(genre_mapping))\n",
    "\n",
    "# Write to txt file\n",
    "plot_dataset_df= raw_dataset_df[[\"genre\",\"plot_description\"]].copy()\n",
    "plot_dataset= plot_dataset_df.to_csv('Data\\\\netflix_plot_dataset.txt', index=False, header=None, sep=\" \")\n",
    "\n",
    "# Sense check - view data header and check all descriptions were mapped\n",
    "print(plot_dataset_df.head())\n",
    "print(plot_dataset_df.genre.unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i9wBQaVAd_Gj",
    "outputId": "53d718cf-f40b-46ee-d476-783b7989e126"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 12 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50269, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Load model and Tokenizer\n",
    "configuration = GPT2Config.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "configuration.pad_token_id = configuration.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model= GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
    "model= model.to(device) \n",
    "\n",
    "\n",
    "special_tokens_dict = {\n",
    "                \"bos_token\": \"<|startoftext|>\",\n",
    "                \"eos_token\": \"<|endoftext|>\",\n",
    "                \"additional_special_tokens\": [\n",
    "                    \"<romance>\",\n",
    "                    \"<drama>\",\n",
    "                    \"<comedy>\",\n",
    "                    \"<documentary>\",\n",
    "                    \"<action>\",\n",
    "                    \"<international>\",\n",
    "                    \"<children>\",\n",
    "                    \"<crime>\",\n",
    "                    \"<horror>\",\n",
    "                    \"<anime>\",\n",
    "                    \"<other>\",\n",
    "                ],\n",
    "            }\n",
    "\n",
    "num_of_toks= tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print('We have added', num_of_toks, 'tokens')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FIhFIX0rd_Gm"
   },
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class NetflixPlotDataset(Dataset):\n",
    "          def __init__(self, tokenizer=tokenizer, dataset_path=os.path.join(\"Data\\\\netflix_plot_dataset.txt\"), block_size=768): # block_size missing\n",
    "   \n",
    "              with open(dataset_path, encoding=\"utf-8\") as f:\n",
    "                      lines = [\"<|startoftext|>\"+line+\"<|endoftext|>\" for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
    "\n",
    "              self.input_ids = tokenizer.batch_encode_plus(lines, add_special_tokens=True, max_length=block_size, truncation=True, padding='max_length')[\"input_ids\"] \n",
    "                \n",
    "        \n",
    "          def __len__(self):\n",
    "            return len(self.input_ids)\n",
    "\n",
    "          def __getitem__(self, i):\n",
    "             return torch.tensor(self.input_ids[i], dtype=torch.long)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W_rVxQDzd_Go",
    "outputId": "76052db6-d62b-4907-ef01-bd33dd099bca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate dataset and return length\n",
    "dataset=NetflixPlotDataset()\n",
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YqUWNRhdd_Gp"
   },
   "outputs": [],
   "source": [
    "def split_datasets_and_create_dataloaders(dataset):\n",
    "  \n",
    "  training_validation_proportion = int(0.8 * len(dataset))\n",
    "                              \n",
    "  train_valid_dataset, test_dataset = random_split(dataset, [training_validation_proportion, len(dataset)- training_validation_proportion])\n",
    "                              \n",
    "  validation_proportion = int(0.25 * len(train_valid_dataset))\n",
    "\n",
    "  training_dataset, validation_dataset = random_split(train_valid_dataset, [len(train_valid_dataset) - validation_proportion, validation_proportion])\n",
    "\n",
    "  print(\"Number of Testing samples: \", len(test_dataset))\n",
    "  print(\"Number of Validation samples: \", len(validation_dataset) )\n",
    "  print(\"Number of Training samples: \", len(training_dataset))\n",
    "\n",
    "  train_dataloader = DataLoader(\n",
    "              training_dataset,\n",
    "              batch_size = 5,\n",
    "              shuffle = True\n",
    "          )\n",
    "\n",
    "  validation_dataloader = DataLoader(\n",
    "              validation_dataset,\n",
    "              batch_size = 5,\n",
    "              shuffle = True\n",
    "          )\n",
    "  \n",
    "  dataloaders = {'train_dataloader': train_dataloader, 'validation_dataloader': validation_dataloader}\n",
    "  \n",
    "  return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9-HTgnK6d_Gr",
    "outputId": "870c138d-e03c-42cf-c9e3-7b3b5b95df62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Testing samples:  20\n",
      "Number of Validation samples:  19\n",
      "Number of Training samples:  60\n"
     ]
    }
   ],
   "source": [
    "dataloaders = split_datasets_and_create_dataloaders(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "jmdakcJ4UbuO"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, checkpoint_path):\n",
    "  print(\"Saving checkpoint ... \")\n",
    "  torch.save(state, checkpoint_path)\n",
    "  print(\"Checkpoint:\", checkpoint_path, \"saved.\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, load_checkpoint_path):\n",
    "  print(\"Loading checkpoint ... \")\n",
    "  checkpoint = torch.load(load_checkpoint_path)\n",
    "  start_epoch = checkpoint['epoch']\n",
    "  model.load_state_dict(checkpoint['state_dict'])\n",
    "  scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "  return model, optimizer, scheduler, start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "t5RwWgRvXr2x"
   },
   "outputs": [],
   "source": [
    "def format_time(start_time,end_time):\n",
    "  hours, remainder = divmod(end_time - start_time, 3600)\n",
    "  minutes, seconds = divmod(remainder, 60)\n",
    "  return (\"{:0>2}:{:0>2}:{:0>2}\".format(int(hours), int(minutes), int(seconds)))\n",
    "\n",
    "def set_seed():\n",
    "  random.seed(100)\n",
    "  np.random.seed(100)\n",
    "  torch.manual_seed(100)\n",
    "  torch.cuda.manual_seed_all(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "XwH8cFzSd_Gy"
   },
   "outputs": [],
   "source": [
    "def training_and_validation(model, optimizer, scheduler, dataloaders, starting_epoch, epochs = 5):\n",
    "  \n",
    "  print(\"\\n\\n\" + \"-\" * 15)\n",
    "  print(\"| TRAINING... |\")\n",
    "  print(\"-\" * 15)\n",
    "  set_seed()\n",
    "\n",
    "  for epoch in range(starting_epoch, epochs):\n",
    "      print(\"\")\n",
    "      print('Training...')\n",
    "      print(\"Epoch:\", epoch + 1, \"/\", epochs )\n",
    "      \n",
    "\n",
    "      start_training_time = time.time()\n",
    "      training_loss = 0\n",
    "      model.train()\n",
    "\n",
    "      for step, batch in enumerate(dataloaders['train_dataloader']):\n",
    "          input_ids = batch[0].to(device)\n",
    "          model.zero_grad()        \n",
    "          outputs = model(input_ids, labels=input_ids)\n",
    "\n",
    "          loss = outputs[0]\n",
    "          batch_loss = loss.item()  \n",
    "          training_loss += batch_loss\n",
    "\n",
    "          if step % 10 == 0 and step != 0:\n",
    "              print(\"Batch \", step, \"/\", len(dataloaders['train_dataloader']), \", Loss: \", batch_loss) \n",
    "\n",
    "              model.eval()\n",
    "              if step % 250 == 0 and step != 0:\n",
    "                samples = model.generate(\n",
    "                                        bos_token_id=random.randint(1,30000),\n",
    "                                        do_sample=True,   \n",
    "                                        top_k=50,\n",
    "                                        max_length = 200,\n",
    "                                        top_p = 0.95,\n",
    "                                        num_return_sequences=1,\n",
    "                                        no_repeat_ngram_size = 2,\n",
    "                                    )\n",
    "\n",
    "                for i, sample in enumerate(samples):\n",
    "                  print(\"{}\".format(tokenizer.decode(sample, skip_special_tokens=True)))\n",
    "              \n",
    "              model.train()\n",
    "\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          scheduler.step()\n",
    "\n",
    "      epoch_loss = training_loss / len(dataloaders['train_dataloader'])       \n",
    "      end_epoch_time = time.time()\n",
    "      epoch_time = format_time(start_training_time, end_epoch_time)\n",
    "\n",
    "      print(\"Epoch Training time: \", epoch_time)\n",
    "      print(\"\")\n",
    "      print(\"Mean Training loss: \", epoch_loss)\n",
    "\n",
    "      print(\"\")\n",
    "      print(\"Validating...\")\n",
    "\n",
    "      start_validation_time = time.time()\n",
    "      \n",
    "      model.eval()\n",
    "      validation_loss = 0\n",
    "      validation_steps = 0\n",
    "\n",
    "      for batch in dataloaders['validation_dataloader']:\n",
    "          input_ids = batch[0].to(device)\n",
    "          \n",
    "          with torch.no_grad():        \n",
    "\n",
    "              outputs  = model(input_ids, labels=input_ids)          \n",
    "              loss = outputs[0]\n",
    "              batch_loss = loss.item()  \n",
    "              \n",
    "          validation_loss += batch_loss        \n",
    "\n",
    "      mean_validation_loss = validation_loss / len(dataloaders['validation_dataloader'])\n",
    "      \n",
    "      end_validation_time = time.time()\n",
    "      epoch_validation_time = format_time(start_validation_time, end_validation_time) \n",
    "      print(\"Validation time: \", epoch_validation_time)  \n",
    "      print(\"Mean Validation Loss: \", mean_validation_loss)\n",
    "\n",
    "      if epoch % 1 == 0:\n",
    "        checkpoint = {\n",
    "          'state_dict': model.state_dict(),\n",
    "          'optimizer': optimizer.state_dict(),\n",
    "          'scheduler': scheduler.state_dict(),\n",
    "          'validation_loss': mean_validation_loss,\n",
    "          'training_loss': epoch_loss,\n",
    "          'epoch': epoch + 1,\n",
    "          }\n",
    "        save_checkpoint(checkpoint, f\"./checkpoint_{checkpoint['epoch']}.pth.tar\")\n",
    "  print(\"\")\n",
    "  print(\"Training Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 715
    },
    "id": "-2XbihZBUZyT",
    "outputId": "adb6d7c5-47da-4204-f8e3-644627d38025"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------------\n",
      "| TRAINING... |\n",
      "---------------\n",
      "\n",
      "Training...\n",
      "Epoch: 1 / 5\n",
      "Batch  10 / 12 , Loss:  5.0036234855651855\n",
      "Epoch Training time:  00:06:26\n",
      "\n",
      "Mean Training loss:  35.321122209231056\n",
      "\n",
      "Validating...\n",
      "Validation time:  00:00:33\n",
      "Mean Validation Loss:  33.611897468566895\n",
      "Saving checkpoint ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python38\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: ./saved/checkpoint_1.pth.tar saved.\n",
      "\n",
      "Training...\n",
      "Epoch: 2 / 5\n",
      "Batch  10 / 12 , Loss:  1.4097317457199097\n",
      "Epoch Training time:  00:05:28\n",
      "\n",
      "Mean Training loss:  2.8084890246391296\n",
      "\n",
      "Validating...\n",
      "Validation time:  00:00:39\n",
      "Mean Validation Loss:  1.2599167078733444\n",
      "Saving checkpoint ... \n",
      "Checkpoint: ./saved/checkpoint_2.pth.tar saved.\n",
      "\n",
      "Training...\n",
      "Epoch: 3 / 5\n",
      "Batch  10 / 12 , Loss:  0.6160365343093872\n",
      "Epoch Training time:  00:04:10\n",
      "\n",
      "Mean Training loss:  0.5852350716789564\n",
      "\n",
      "Validating...\n",
      "Validation time:  00:00:29\n",
      "Mean Validation Loss:  0.3207813985645771\n",
      "Saving checkpoint ... \n",
      "Checkpoint: ./saved/checkpoint_3.pth.tar saved.\n",
      "\n",
      "Training...\n",
      "Epoch: 4 / 5\n",
      "Batch  10 / 12 , Loss:  0.42547041177749634\n",
      "Epoch Training time:  00:04:39\n",
      "\n",
      "Mean Training loss:  0.36747600014011067\n",
      "\n",
      "Validating...\n",
      "Validation time:  00:00:39\n",
      "Mean Validation Loss:  0.29345547035336494\n",
      "Saving checkpoint ... \n",
      "Checkpoint: ./saved/checkpoint_4.pth.tar saved.\n",
      "\n",
      "Training...\n",
      "Epoch: 5 / 5\n",
      "Batch  10 / 12 , Loss:  0.3019256889820099\n",
      "Epoch Training time:  00:04:36\n",
      "\n",
      "Mean Training loss:  0.33079540729522705\n",
      "\n",
      "Validating...\n",
      "Validation time:  00:00:37\n",
      "Mean Validation Loss:  0.2792260870337486\n",
      "Saving checkpoint ... \n",
      "Checkpoint: ./saved/checkpoint_5.pth.tar saved.\n",
      "\n",
      "Training Finished\n"
     ]
    }
   ],
   "source": [
    "# First Training time... no checkpoint\n",
    "\n",
    "steps = len(dataloaders['train_dataloader']) * 5\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                lr = 4e-3,\n",
    "                eps = 1e-8)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                          num_warmup_steps = 1e3, \n",
    "                                          num_training_steps = steps)\n",
    "\n",
    "training_and_validation(model, optimizer, scheduler, dataloaders, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2axHoz-gsroX",
    "outputId": "b446451a-fd06-484d-f803-34f0c0a53804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python38\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------------\n",
      "| TRAINING... |\n",
      "---------------\n",
      "\n",
      "Training...\n",
      "Epoch: 2 / 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-b45945eb9202>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"./saved/checkpoint_1.pth.tar\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtraining_and_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-18965ee19d99>\u001b[0m in \u001b[0;36mtraining_and_validation\u001b[1;34m(model, optimizer, scheduler, dataloaders, starting_epoch, epochs)\u001b[0m\n\u001b[0;32m     45\u001b[0m               \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m           \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m           \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python38\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load checkpoint from last epoch and continue training\n",
    "\n",
    "model, optimizer, scheduler, start_epoch = load_checkpoint(model, optimizer, scheduler, \"./checkpoint_1.pth.tar\")\n",
    "\n",
    "training_and_validation(model, optimizer, scheduler, dataloaders, start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ZNhTgqXFhdA",
    "outputId": "dd774139-2c1d-4e63-db01-68469441fc7c"
   },
   "outputs": [],
   "source": [
    "# Generate Movie Plots\n",
    "\n",
    "model.eval()\n",
    "\n",
    "prompt = \"<|startoftext|> <drama>\"\n",
    "\n",
    "outputs = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "outputs = outputs.to(device)\n",
    "\n",
    "movie_plots = model.generate(outputs, do_sample=True, \n",
    "                             max_length=300, min_length=100,\n",
    "                             top_k=50,  top_p=0.95, \n",
    "                             num_return_sequences=10, no_repeat_ngram_size = 2, \n",
    "                             repetition_penalty = 1.2\n",
    "                             )\n",
    "\n",
    "for index, movie_plot in enumerate(movie_plots):\n",
    "  print(\"\\n Sample movie plot: \", tokenizer.decode(movie_plot, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "INM706_Code_SR_SK (1).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
